{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Setup, Functions\n",
    "from TwitterAPI import TwitterAPI, TwitterPager\n",
    "import yaml\n",
    "import time\n",
    "import json_lines\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "with open(\"config.yaml\", 'r') as ymlfile:\n",
    "    config = yaml.safe_load(ymlfile)\n",
    "\n",
    "api = TwitterAPI(config['twitter']['api_key'], \n",
    "                 config['twitter']['api_secret_key'],\n",
    "                 auth_type='oAuth2'\n",
    "                )\n",
    "\n",
    "\n",
    "# Collect full Tweets\n",
    "def collect_tweets(search_term, since='', since_id='', filename='', max_id=''):\n",
    "    print(search_term)\n",
    "    tweets = []\n",
    "    if since != '':\n",
    "        query = '{0} since:{1}'.format(search_term, since)\n",
    "    else:\n",
    "        query = search_term\n",
    "    r = TwitterPager(api, 'search/tweets', {'q': query, \n",
    "                                            'count':100, \n",
    "                                            'tweet_mode':'extended', \n",
    "                                            'since_id':since_id,\n",
    "                                            'max_id':max_id,\n",
    "                                            'result_type':'recent'})\n",
    "    n = 0\n",
    "    if filename == '':\n",
    "        filename = search_term +'.jsonl'\n",
    "        \n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        for item in r.get_iterator(wait=2):\n",
    "            n += 1\n",
    "            if n % 1000 == 0:\n",
    "                print(item['created_at'] + ' ' + str(n))\n",
    "            if 'full_text' in item:\n",
    "                json.dump(item, f)\n",
    "                f.write('\\n')\n",
    "            elif 'message' in item and item['code'] == 88:\n",
    "                print ('SUSPEND, RATE LIMIT EXCEEDED: %s\\n' % item['message'])\n",
    "                break\n",
    "        return tweets\n",
    "\n",
    "# Collect only IDs and meta data of Tweets\n",
    "def collect_tweet_ids(search_term, since='', since_id='', filename='', max_id=''):\n",
    "    print(search_term)\n",
    "    if since != '':\n",
    "        query = '{0} since:{1}'.format(search_term, since)\n",
    "    else:\n",
    "        query = search_term\n",
    "    r = TwitterPager(api, 'search/tweets', {'q': query, \n",
    "                                            'count':100, \n",
    "                                            'since_id':since_id,\n",
    "                                            'max_id':max_id,\n",
    "                                            'result_type':'recent'})\n",
    "    n = 0\n",
    "    if filename == '':\n",
    "        filename = search_term +'.jsonl'\n",
    "        \n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        for item in r.get_iterator(wait=2.2):\n",
    "            n += 1\n",
    "            if n % 1000 == 0:\n",
    "                print(item['created_at'] + ' ' + str(n))\n",
    "            if 'text' in item:\n",
    "                json.dump({'created_at' : item['created_at'],\n",
    "                           'id' : item['id'],\n",
    "                           'user_id' : item['user']['id'],\n",
    "                           'at_id' : item['in_reply_to_status_id'] if 'in_reply_to_status_id' in item else None,\n",
    "                           'rt_id' : item['retweeted_status']['id'] if 'retweeted_status' in item else None,\n",
    "                           'qt_id' : item['quoted_status']['id'] if 'quoted_status' in item else None\n",
    "                          }, f)\n",
    "                f.write('\\n')\n",
    "            elif 'message' in item and item['code'] == 88:\n",
    "                print ('SUSPEND, RATE LIMIT EXCEEDED: %s\\n' % item['message'])\n",
    "                break\n",
    "        return ()\n",
    "    \n",
    "\n",
    "def hydrate(ids, filename):\n",
    "    n = 0\n",
    "    chunks = [ids[x:x+100] for x in range(0, len(ids), 100)]\n",
    "    while len(chunks) > 0:\n",
    "        time.sleep(3) #300 requests per 15min\n",
    "        chunk = chunks.pop()\n",
    "        r = api.request('statuses/lookup', {'id': ','.join(map(str,chunk)),\n",
    "                                                  'count':100,\n",
    "                                                  'tweet_mode':'extended'\n",
    "                                           })\n",
    "    \n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            for item in r:\n",
    "                n += 1\n",
    "                if n % 100 == 0:\n",
    "                    print(item['created_at'] + ' ' + str(n))\n",
    "                if 'full_text' in item:\n",
    "                    json.dump(item, f)\n",
    "                    f.write('\\n')\n",
    "                elif 'message' in item and item['code'] == 88:\n",
    "                    print ('SUSPEND, RATE LIMIT EXCEEDED: %s\\n'.format(item['message']))\n",
    "                    break\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_tweets(tweets, file):\n",
    "    with open(file, 'a', encoding='utf-8') as f:\n",
    "        n = 0\n",
    "        for tweet in tweets:\n",
    "            n += 1\n",
    "            if n % 1000 == 0:\n",
    "                print(tweet['created_at'] + ' ' + str(n))\n",
    "            json.dump(tweet, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "\n",
    "def load_tweet_ids_from_jsonl(files):\n",
    "    ids = set()\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'rb') as f:\n",
    "            for tweet in json_lines.reader(f, broken=True):\n",
    "                ids.add(tweet['id'])\n",
    "    return (ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest ID of last collection\n",
    "ids = load_tweet_ids_from_jsonl(['lang_de-2020-11-28_IDs.jsonl'])\n",
    "\n",
    "idslist = list(ids)\n",
    "idslist.sort()\n",
    "highest_id = idslist[-1]\n",
    "print(highest_id)\n",
    "\n",
    "del(ids)\n",
    "del(idslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start new collection\n",
    "with open('collection_log.txt', 'a') as f:\n",
    "    f.write(f'{datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d %H:%M:%S\")} Collection started \\n')\n",
    "\n",
    "collect_tweet_ids('lang:de',\n",
    "                  filename='lang_de-2020-11-30_IDs.jsonl',\n",
    "                  since_id = highest_id\n",
    "                 )\n",
    "with open('collection_log.txt', 'a') as f:\n",
    "    f.write(f'{datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d %H:%M:%S\")} Collection finished\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
